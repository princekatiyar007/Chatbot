import React, { useRef, useState } from 'react';

const StreamAudioPlayer = () => {
  const audioRef = useRef(null);

  const [query, setQuery] = useState('');
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [isPlaying, setIsPlaying] = useState(false);

  const submitHandler = async (e) => {
    e.preventDefault();
    setLoading(true);
    setError(null);
    setIsPlaying(false);

    try {
      // Step 1: Call the first API with the user input
      const api1Response = await fetch('http://localhost:5000/process-input', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query }),
      });

      if (!api1Response.ok) throw new Error('Failed to process input');

      const api1Data = await api1Response.json();
      const transcriptOrPrompt = api1Data.processedText; // Adjust this based on actual API response key

      // Step 2: Call the second API to stream audio from processed text
      const mediaSource = new MediaSource();
      const audioEl = audioRef.current;
      audioEl.src = URL.createObjectURL(mediaSource);

      mediaSource.addEventListener('sourceopen', async () => {
        const sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg'); // or 'audio/wav'

        const api2Response = await fetch('http://localhost:5000/audio-stream', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ input: transcriptOrPrompt }),
        });

        if (!api2Response.body) throw new Error('No audio stream available');

        const reader = api2Response.body.getReader();

        const pump = async () => {
          const { done, value } = await reader.read();

          if (done) {
            mediaSource.endOfStream();
            setLoading(false);
            setIsPlaying(true);
            return;
          }

          sourceBuffer.appendBuffer(value);
          sourceBuffer.addEventListener('updateend', pump, { once: true });
        };

        pump();
      });
    } catch (err) {
      console.error('Error during audio generation:', err);
      setError('Something went wrong while generating audio.');
      setLoading(false);
    }
  };

  return (
    <div className="max-w-md mx-auto p-4">
      <h2 className="text-xl font-bold mb-4 text-center">üîÅ Chained Audio Generator</h2>

      <form onSubmit={submitHandler} className="mb-4">
        <input
          type="text"
          className="border border-gray-400 rounded px-3 py-2 w-full mb-2"
          placeholder="Enter query..."
          value={query}
          onChange={(e) => setQuery(e.target.value)}
          required
        />
        <button
          type="submit"
          className="bg-blue-600 text-white px-4 py-2 rounded w-full hover:bg-blue-700 transition"
        >
          Generate Audio
        </button>
      </form>

      {loading && <p className="text-blue-500 text-center">‚è≥ Generating & streaming audio...</p>}
      {error && <p className="text-red-500 text-center">{error}</p>}
      {isPlaying && <p className="text-green-500 text-center">üé∂ Audio is playing now</p>}

      <audio ref={audioRef} controls className="mt-4 w-full" />
    </div>
  );
};

export default StreamAudioPlayer;






import React, { useEffect, useRef } from 'react';

const StreamAudioPlayer = () => {
  const audioRef = useRef(null);

  useEffect(() => {
    const mediaSource = new MediaSource();
    const audioEl = audioRef.current;

    audioEl.src = URL.createObjectURL(mediaSource);

    mediaSource.addEventListener("sourceopen", async () => {
      const sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg'); // or 'audio/wav' based on backend

      try {
        const response = await fetch('http://localhost:5000/audio-stream'); // your stream endpoint
        const reader = response.body.getReader();

        const pump = async () => {
          const { done, value } = await reader.read();

          if (done) {
            mediaSource.endOfStream();
            return;
          }

          // Append chunk to buffer
          sourceBuffer.appendBuffer(value);
          sourceBuffer.addEventListener("updateend", pump, { once: true });
        };

        pump();
      } catch (err) {
        console.error("Streaming error:", err);
      }
    });
  }, []);

  return (
    <div className="p-4">
      <h2 className="text-xl font-bold mb-2">Streaming Audio Player</h2>
      <audio ref={audioRef} controls autoPlay />
    </div>
  );
};

export default StreamAudioPlayer;










Certainly! Here's a **greatly detailed and humanized weekly report** of your Samsung Delhi internship, with **expanded bullet points** for clarity and depth, crafted to reflect your personal learning journey in a professional tone.

---

# **üóìÔ∏è Weekly Internship Report ‚Äì Samsung R\&D Institute, Delhi**

**Intern Name**: Prince Katiyar
**Internship Duration Covered**: *13th May ‚Äì 4th July 2025*
**Department**: MY Enterprises
**Projects**: AI in Education, Saathi App, Podcast Generator

---

## ‚úÖ **Week 1: 13th May ‚Äì 16th May**

### *Induction & Project Assignment Phase*

* **Orientation and Induction**:

  * Attended a comprehensive two-day induction program covering all functional, cultural, and administrative aspects of Samsung Delhi.
  * Learnt about Samsung‚Äôs **work ethics, code of conduct, leave policies, holiday structure**, and internal security protocols.
  * Understood core principles around **data protection, user privacy**, and potential risks of **data breach** with real-world examples from past case studies.
  * Got introduced to Samsung‚Äôs proprietary tools and internal platforms for communication, documentation, and project tracking.

* **Team Integration and Project Introduction**:

  * Officially onboarded into the **MY Enterprises team**, known for its innovation-driven projects.
  * Met team leads, mentors, and senior developers‚Äîestablished communication channels (Slack, Confluence, internal mail).
  * Participated in a **team meeting** where I was assigned to the project titled **"AI in Education"**.

* **Project Understanding: AI in Education**:

  * Project objective: Transform recorded teacher lectures into **blog-style learning content** for student use.
  * Workflow explained:

    * Record classroom audio
    * Convert speech to **transcript**
    * Analyze transcript to generate **structured summary** in the format of a blog (with headings, subheadings, bullets, images).
  * Tools to be used: **Google Agent Development Kit (ADK)** for building intelligent agents.
  * Long-term goal: Create a **multi-agent framework** that can autonomously convert transcripts to blog posts end-to-end.

---

## ‚úÖ **Week 2: 19th May ‚Äì 25th May**

### *Foundation Phase: Learning ADK & First Agent Development*

* **Self-Learning & Research**:

  * Started studying the **Google Agent Development Kit (ADK)** through its official documentation and sample code.
  * Supplemented reading with **YouTube tutorials** to grasp:

    * What agentic AI is.
    * How intelligent agents work under the hood.
    * How agents interact with different models and pipelines.

* **Hands-On: First AI Agent**:

  * Built my **first working agent** named `get_weather_agent`.

    * Function: Takes city name as input and returns **real-time weather info**.
    * Model used: **Ollama** (suitable for lightweight NLP tasks).
    * Gained understanding of how models interact with user prompts via agents.
    * Learned how to define **intents, slots, and outputs** in agent configuration.
  * Identified limitations: Ollama is not suitable for tasks requiring reasoning or large-scale text generation.

* **Project Structuring into Sub-Agents**:

  * In a follow-up team meeting, the project was divided into **three distinct agent modules**:

    1. **Heading Generator Agent** ‚Äì to extract blog headings from transcripts.
    2. **Content Generator Agent** ‚Äì to generate meaningful paragraph content.
    3. **Evaluator Agent** ‚Äì to ensure logical structure, quality, and coherence.
  * I was officially assigned the **Heading Generator Agent** module for development.

---

## ‚úÖ **Week 3: 26th May ‚Äì 30th May**

### *Advanced Agent Development with Qwen Model*

* **Model Upgrade**:

  * Granted access to the more powerful **Qwen language model**.
  * Integrated Qwen into the **Heading Generator Agent**, achieving higher quality and more contextually accurate headings from sample transcripts.

* **Studying Agent Architectures**:

  * Deep-dived into the concepts of:

    * **Custom Agents**: Configurable agent templates for specialized tasks.
    * **Parallel Agents**: Simultaneously executing multiple agents for performance.
    * **Sequential Agents**: Pipeline-style agents for ordered processing (used in our project).
  * Understood how to pass outputs from one agent as inputs to the next using Google ADK‚Äôs chaining mechanism.

* **Custom Agent Development**:

  * Built a **Custom Controller Agent** to manage the sequential execution of:

    * Heading extraction ‚Üí Content generation ‚Üí Evaluation.
  * This agent acts like a mini-orchestrator ensuring the workflow of sub-agents is aligned and responsive.

---

## ‚úÖ **Week 4: 2nd June ‚Äì 6th June**

### *Image Generation & App Documentation*

* **Gemini Model Integration**:

  * Got access to **Google‚Äôs Gemini model** (multimodal AI).
  * Developed an **Image Generation Agent**:

    * Parses blog content and suggests **visually relevant images**.
    * Helps bring aesthetic and contextual value to generated blogs.

* **Contributing to Saathi App**:

  * Worked on **Saathi App** (quiz generator) alongside my primary project.
  * Conducted an in-depth UX and feature analysis:

    * Documented **functionalities**, **usability bugs**, and **logical inconsistencies**.
    * Shared structured documentation on **Confluence**, categorized as:

      * UI Issues
      * Functional Gaps
      * Suggested Improvements

---

## ‚úÖ **Week 5: 9th June ‚Äì 13th June**

### *Frontend Development Begins*

* **Exploring Blog UIs**:

  * Spent time analyzing **modern blog UIs** to understand content layout, responsiveness, and readability.
  * Created low-fidelity designs and layout mockups using **MS Word**.

* **Learning Tailwind CSS**:

  * Understood utility-first design principles of **Tailwind CSS**.
  * Practiced key concepts like:

    * Grid layouts
    * Flexbox containers
    * Responsive design classes
    * Hover effects, transitions

* **Initial Blogify UI Setup**:

  * Created a basic blog webpage using **local JSON data** as placeholder content.
  * Began styling the blog using Tailwind CSS with mobile-first responsiveness.
  * Designed components: header, title, content section, image blocks, bullet list cards.

---

## ‚úÖ **Week 6: 16th June ‚Äì 20th June**

### *Backend Integration & Parallel Work on Podcast Generator*

* **Learning Data Integration**:

  * Studied documentation and tutorials on:

    * REST API structure
    * GET and POST methods
    * Axios and Fetch in React
  * Learned how to create a simple **Node.js backend server** to send and receive data.

* **Frontend-Backend Integration**:

  * Integrated backend data (transcripts, headings, image links) into the **Blogify frontend**.
  * Ensured **real-time dynamic rendering** of blog pages based on user query or class transcript.

* **SWC Exam Prep Begins**:

  * Started solving 3‚Äì4 **DSA questions daily** from Expert Academy.
  * Watched curated videos covering sorting, recursion, and greedy algorithms.

* **New Assignment: Podcast Generator**:

  * Joined another project where the goal is to **generate AI-based podcasts** using classroom transcript as context.
  * Assigned frontend responsibilities.
  * Understood app flow: Transcript ‚Üí Dialogue ‚Üí Voice Cloning ‚Üí Podcast.

---

## ‚úÖ **Week 7: 23rd June ‚Äì 27th June**

### *UI Refinements, Real-Time Image Handling & Continued Learning*

* **Enhanced Blogify Frontend**:

  * Focused on fixing challenges related to **real-time image rendering**:

    * Managed image orientation and scaling.
    * Applied Tailwind CSS classes to ensure visual harmony with text.

* **Podcast Generator Frontend**:

  * Started developing frontend from scratch using **React.js** and **Tailwind CSS**.
  * Learnt React concepts:

    * Components
    * Props & State
    * JSX Syntax
  * Practiced by building simple components like input boxes, buttons, and audio players.

* **Continued DSA Practice**:

  * Solved problems based on:

    * Two pointers
    * Sliding window
    * Linked lists
  * Balanced coding with frontend implementation efforts.

---

## ‚úÖ **Week 8: 30th June ‚Äì 4th July**

### *Podcast Generator Completion & Demos*

* **Podcast Generator Completion**:

  * Completed building a **React-based podcast UI**.
  * Functionality:

    * Takes a transcript input
    * Fetches and plays audio using backend APIs
    * Displays interactive audio player UI
  * Used `audio/wav` format and ensured compatibility across browsers.

* **Live Demonstrations**:

  * Participated in **team demos** for:

    * Podcast Generator: Showcased full functionality to mentors and senior managers.
    * Blogify App: Walked through UI design and backend integration.

* **Final UI Polishing**:

  * Spent last couple of days on:

    * Enhancing responsiveness
    * Refining fonts, padding, layout spacing
    * Handling edge cases in content rendering (long paragraphs, image alignment, etc.)

---

## üìå **Overall Achievements (So Far)**

* Built **4 AI agents** using Google ADK.
* Contributed to **3 full-stack applications**.
* Learned and implemented:

  * React.js, Tailwind CSS, API Integration, Server Setup
  * Agent-based architecture and prompt workflows
* Participated in team meetings, **demos**, and **documentation cycles**.
* Balanced technical work with **competitive programming prep** for SWC.

---

Let me know if you‚Äôd like this converted to a **PDF**, formatted into a **PowerPoint**, or prepared for a **resume/brochure**.































<div id="content" class="space-y-6"></div>

<script>
  const content = [
    { type: "text", value: "This is a paragraph" },
    { type: "image", value: "https://via.placeholder.com/300" },
    { type: "text", value: "Another paragraph" },
    { type: "image", value: "https://via.placeholder.com/400" }
  ];

  const container = document.getElementById("content");

  function addFullscreenButtonToImage(img) {
    const wrapper = document.createElement("div");
    wrapper.className = "relative inline-block w-fit";

    img.classList.add(
      "rounded-xl",
      "shadow-lg",
      "transition",
      "hover:scale-105",
      "duration-300",
      "cursor-pointer"
    );

    img.setAttribute("loading", "lazy");

    wrapper.appendChild(img);

    const button = document.createElement("button");
    button.innerHTML = "üîç";
    button.title = "Toggle Fullscreen";
    button.className =
      "absolute top-2 right-2 bg-black bg-opacity-50 p-2 rounded-full text-white hover:bg-opacity-75 z-10";

    let isFullscreen = false;

    button.onclick = () => {
      if (!isFullscreen) {
        if (img.requestFullscreen) img.requestFullscreen();
        else if (img.webkitRequestFullscreen) img.webkitRequestFullscreen();
        else if (img.msRequestFullscreen) img.msRequestFullscreen();
      } else {
        if (document.exitFullscreen) document.exitFullscreen();
        else if (document.webkitExitFullscreen) document.webkitExitFullscreen();
        else if (document.msExitFullscreen) document.msExitFullscreen();
      }
    };

    document.addEventListener("fullscreenchange", () => {
      isFullscreen = !!document.fullscreenElement;
      button.innerHTML = isFullscreen ? "‚ùå" : "üîç";
    });

    wrapper.appendChild(button);
    container.appendChild(wrapper);
  }

  content.forEach(item => {
    if (item.type === "text") {
      const p = document.createElement("p");
      p.textContent = item.value;
      p.className = "text-gray-800 text-base";
      container.appendChild(p);
    } else if (item.type === "image") {
      const img = document.createElement("img");
      img.src = item.value;
      img.alt = "dynamic image";
      img.className = "max-w-full";
      addFullscreenButtonToImage(img);
    }
  });
</script>








<script>
  // Function to add a fullscreen button to an image
  function addFullscreenButton(imageId) {
    const image = document.getElementById(imageId);
    const container = image.parentElement;

    // Create button
    const button = document.createElement("button");
    button.innerHTML = "üîç"; // Or replace with SVG icon
    button.title = "View Fullscreen";

    // Style the button with Tailwind classes
    button.className =
      "absolute top-2 right-2 bg-black bg-opacity-50 p-2 rounded-full text-white hover:bg-opacity-75";

    // Add click event
    button.onclick = () => {
      if (image.requestFullscreen) {
        image.requestFullscreen();
      } else if (image.webkitRequestFullscreen) {
        image.webkitRequestFullscreen(); // Safari
      } else if (image.msRequestFullscreen) {
        image.msRequestFullscreen(); // IE
      }
    };

    // Ensure container has relative positioning
    if (!container.classList.contains("relative")) {
      container.classList.add("relative");
    }

    // Append the button
    container.appendChild(button);
  }

  // Call the function after DOM is loaded
  addFullscreenButton("myImage");
</script>








import React, { useState } from "react";

function AudioGenerator() {
  const [topic, setTopic] = useState("");
  const [audioUrl, setAudioUrl] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState("");

  const handleSubmit = async (e) => {
    e.preventDefault();
    setLoading(true);
    setError("");
    setAudioUrl(null);

    try {
      // 1Ô∏è‚É£ First API: Get Script
      const res1 = await fetch("https://api1.example.com", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ topic }),
      });
      const data1 = await res1.json();
      const script = data1.result; // adjust to actual key

      // 2Ô∏è‚É£ Second API: Get Audio
      const res2 = await fetch("https://api2.example.com", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ script }),
      });

      if (!res2.ok) throw new Error("Audio generation failed");

      const audioBlob = await res2.blob();
      const audioObjectUrl = URL.createObjectURL(audioBlob);
      setAudioUrl(audioObjectUrl);
    } catch (err) {
      setError("Something went wrong: " + err.message);
      console.error(err);
    }

    setLoading(false);
  };

  return (
    <div className="min-h-screen bg-gradient-to-br from-gray-900 via-gray-800 to-gray-900 text-white flex items-center justify-center p-4">
      <div className="bg-gray-800 rounded-2xl shadow-2xl p-8 w-full max-w-2xl">
        <h1 className="text-3xl font-bold mb-6 text-center">üé§ Script to Audio Generator</h1>

        <form onSubmit={handleSubmit} className="space-y-4">
          <textarea
            className="w-full p-4 h-32 rounded-lg bg-gray-700 border border-gray-600 focus:outline-none focus:ring-2 focus:ring-blue-500 resize-none text-white placeholder-gray-400"
            placeholder="Enter your topic here..."
            value={topic}
            onChange={(e) => setTopic(e.target.value)}
          />

          <button
            type="submit"
            className="w-full bg-blue-600 hover:bg-blue-700 transition duration-200 text-white font-bold py-3 rounded-lg"
            disabled={loading}
          >
            {loading ? "Generating Audio..." : "Submit"}
          </button>
        </form>

        {error && (
          <div className="mt-4 text-red-400 text-center font-medium">{error}</div>
        )}

        {audioUrl && (
          <div className="mt-8">
            <h2 className="text-xl font-semibold mb-2 text-center">üîä Your Audio Output</h2>
            <audio
              className="w-full mt-2 rounded-lg"
              controls
              src={audioUrl}
            />
          </div>
        )}
      </div>
    </div>
  );
}

export default AudioGenerator;







module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};







/** @type {import('tailwindcss').Config} */
module.exports = {
  content: ["./src/**/*.{js,jsx,ts,tsx}"],
  theme: {
    extend: {},
  },
  plugins: [],
};







import React, { useState } from "react";

function AudioGenerator() {
  const [topic, setTopic] = useState("");
  const [audioUrl, setAudioUrl] = useState(null);
  const [loading, setLoading] = useState(false);

  const handleSubmit = async (e) => {
    e.preventDefault();
    setLoading(true);
    setAudioUrl(null);

    try {
      // üîπ Step 1: Send topic to API 1
      const res1 = await fetch("https://api1.example.com", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ topic }),
      });

      const data1 = await res1.json();
      const script = data1.result; // Adjust based on API 1's response format

      // üîπ Step 2: Send script to API 2 and get audio stream
      const res2 = await fetch("https://api2.example.com", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ script }),
      });

      if (!res2.ok) {
        throw new Error("Failed to get audio");
      }

      const audioBlob = await res2.blob(); // Convert audio stream to blob
      const audioObjectUrl = URL.createObjectURL(audioBlob); // Convert blob to playable URL
      setAudioUrl(audioObjectUrl); // Set URL for audio player

    } catch (error) {
      console.error("Error generating audio:", error);
    }

    setLoading(false);
  };

  return (
    <div className="p-8 max-w-xl mx-auto">
      <form onSubmit={handleSubmit}>
        <textarea
          value={topic}
          onChange={(e) => setTopic(e.target.value)}
          placeholder="Enter topic"
          className="w-full h-32 p-2 border border-gray-400 rounded mb-4"
        />
        <button
          type="submit"
          className="bg-black text-white px-4 py-2 rounded hover:bg-gray-800"
        >
          Generate Audio
        </button>
      </form>

      {loading && <p className="mt-4">Generating audio...</p>}

      {audioUrl && (
        <div className="mt-6">
          <audio controls src={audioUrl} className="w-full" />
        </div>
      )}
    </div>
  );
}

export default AudioGenerator;





import React from "react";
import "./index.css"; // optional, use for Tailwind/custom styles
import HomePage from "./HomePage";

function App() {
  return <HomePage />;
}

export default App;






import React, { useState } from "react";

function HomePage() {
  const [topic, setTopic] = useState("");
  const [response, setResponse] = useState("");

  const handleSubmit = async (e) => {
    e.preventDefault();

    try {
      const res = await fetch("https://your-api.com/endpoint", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ topic: topic }),
      });

      const data = await res.json();
      setResponse(data.result); // Change this based on actual API response
    } catch (err) {
      console.error("Error:", err);
      setResponse("Something went wrong.");
    }
  };

  return (
    <div className="p-8 max-w-xl mx-auto">
      <form onSubmit={handleSubmit}>
        <textarea
          value={topic}
          onChange={(e) => setTopic(e.target.value)}
          placeholder="Enter topic here"
          className="w-full h-32 p-2 border border-gray-400 rounded mb-4"
        />
        <button
          type="submit"
          className="bg-black text-white px-4 py-2 rounded hover:bg-gray-800"
        >
          Submit
        </button>
      </form>

      {response && (
        <div className="mt-6 p-4 border border-gray-300 rounded bg-gray-50">
          <h3 className="font-bold">Response:</h3>
          <p>{response}</p>
        </div>
      )}
    </div>
  );
}

export default HomePage;















<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Input Page</title>
</head>
<body>
  <h2>Enter Query</h2>
  <input type="text" id="queryInput" placeholder="Type something..." />
  <button onclick="submitQuery()">Submit</button>

  <script src="js/index.js"></script>
</body>
</html>




<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Result Page</title>
</head>
<body>
  <h2>Processed Output</h2>
  <p id="displayQuery"></p>

  <script src="js/index.js"></script>
</body>
</html>




// Detect current page
const path = window.location.pathname;

// Page 1: index.html ‚Äì just redirect with raw input
if (path.endsWith("index.html")) {
  window.submitQuery = function () {
    const rawQuery = document.getElementById("queryInput").value.trim();
    if (rawQuery) {
      // Pass raw input to result page (not processed yet)
      const encoded = encodeURIComponent(rawQuery);
      window.location.href = `result.html?q=${encoded}`;
    }
  };
}

// Page 2: result.html ‚Äì receive raw input and process it
if (path.endsWith("result.html")) {
  const params = new URLSearchParams(window.location.search);
  const rawQuery = params.get("q");

  const displayEl = document.getElementById("displayQuery");
  if (rawQuery) {
    const processed = processQuery(rawQuery); // Process here
    displayEl.innerText = `Processed: ${processed}`;
  } else {
    displayEl.innerText = "No query provided.";
  }
}

// Example processing function
function processQuery(input) {
  // Transform raw input (e.g., lowercase ‚Üí uppercase)
  return input.toUpperCase();
}











import React, { useEffect, useRef, useState } from 'react';
import WaveSurfer from 'wavesurfer.js';

const Waveform = ({ audioUrl }) => {
  const waveformRef = useRef(null);
  const wavesurferRef = useRef(null);
  const [isPlaying, setIsPlaying] = useState(false);

  useEffect(() => {
    if (!waveformRef.current) return;

    // Cleanup old instance if re-rendered
    if (wavesurferRef.current) {
      wavesurferRef.current.destroy();
    }

    // Initialize WaveSurfer
    wavesurferRef.current = WaveSurfer.create({
      container: waveformRef.current,
      waveColor: '#94a3b8',
      progressColor: '#4ade80',
      cursorColor: '#22c55e',
      height: 100,
      responsive: true,
      barWidth: 2,
    });

    wavesurferRef.current.load(audioUrl);

    return () => {
      wavesurferRef.current.destroy();
    };
  }, [audioUrl]);

  const togglePlay = () => {
    if (wavesurferRef.current) {
      wavesurferRef.current.playPause();
      setIsPlaying(wavesurferRef.current.isPlaying());
    }
  };

  return (
    <div className="w-full max-w-2xl mx-auto mt-6">
      <div ref={waveformRef} className="rounded border border-green-500" />
      <button
        onClick={togglePlay}
        className="mt-3 px-4 py-2 bg-green-600 hover:bg-green-700 text-white font-semibold rounded"
      >
        {isPlaying ? 'Pause' : 'Play'}
      </button>
    </div>
  );
};

export default Waveform;











import React, { useEffect, useRef } from 'react';

const AudioVisualizer = ({ audioRef }) => {
  const canvasRef = useRef(null);
  const isInitialized = useRef(false); // ‚úÖ Track init

  useEffect(() => {
    if (!audioRef.current || isInitialized.current) return;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioCtx.createAnalyser();
    analyser.fftSize = 256;

    try {
      const source = audioCtx.createMediaElementSource(audioRef.current);
      source.connect(analyser);
      analyser.connect(audioCtx.destination);
    } catch (e) {
      console.error('Audio source already connected:', e);
      return;
    }

    isInitialized.current = true; // ‚úÖ Mark as initialized

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const canvas = canvasRef.current;
    const canvasCtx = canvas.getContext('2d');

    const draw = () => {
      requestAnimationFrame(draw);
      analyser.getByteFrequencyData(dataArray);

      canvasCtx.fillStyle = '#111827';
      canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

      const barWidth = (canvas.width / bufferLength) * 2.5;
      let x = 0;

      for (let i = 0; i < bufferLength; i++) {
        const barHeight = dataArray[i] / 2;
        canvasCtx.fillStyle = '#4ade80';
        canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
        x += barWidth + 1;
      }
    };

    draw();

    const resume = () => audioCtx.resume();
    audioRef.current.addEventListener('play', resume);

    return () => {
      if (audioRef.current) {
        audioRef.current.removeEventListener('play', resume);
      }
      audioCtx.close();
    };
  }, [audioRef]);

  return (
    <canvas
      ref={canvasRef}
      width={600}
      height={100}
      className="mt-4 border border-green-500 rounded"
    />
  );
};

export default AudioVisualizer;








import React, { useState, useEffect, useRef } from 'react';
import './index.css';
import InputSection from './components/InputSection';
import SpeakerDisplay from './components/SpeakerDisplay';
import AudioVisualizer from './components/AudioVisualizer';

function App() {
  const [text, setText] = useState('');
  const [speaker, setSpeaker] = useState(null);
  const [voices, setVoices] = useState([]);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const audioRef = useRef(null); // NEW

  useEffect(() => {
    const handleVoicesChanged = () => {
      const availableVoices = speechSynthesis.getVoices();
      setVoices(availableVoices);
    };

    speechSynthesis.addEventListener('voiceschanged', handleVoicesChanged);
    handleVoicesChanged();

    return () => {
      speechSynthesis.removeEventListener('voiceschanged', handleVoicesChanged);
    };
  }, []);

  const handleSubmit = () => {
    if (!text.trim()) return;

    const sentences = text.split(/(?<=[.!?])\s+/);
    let currentSpeaker = 0;

    const maleVoice = voices.find(v =>
      v.name.toLowerCase().includes('david') || v.name.toLowerCase().includes('male')
    );
    const femaleVoice = voices.find(v =>
      v.name.toLowerCase().includes('zira') || v.name.toLowerCase().includes('female')
    );

    const speak = (index) => {
      if (index >= sentences.length) {
        setSpeaker(null);
        setIsSpeaking(false);
        return;
      }

      const utterance = new SpeechSynthesisUtterance(sentences[index]);

      if (currentSpeaker === 0 && maleVoice) {
        utterance.voice = maleVoice;
      } else if (currentSpeaker === 1 && femaleVoice) {
        utterance.voice = femaleVoice;
      }

      setSpeaker(currentSpeaker);
      currentSpeaker = (currentSpeaker + 1) % 2;

      utterance.onstart = () => setIsSpeaking(true);
      utterance.onend = () => {
        setIsSpeaking(false);
        speak(index + 1);
      };

      speechSynthesis.speak(utterance);
    };

    speak(0);
  };

  const handleFileUpload = (e) => {
    const file = e.target.files[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = (e) => {
      setText(e.target.result);
    };
    reader.readAsText(file);
  };

  return (
    <div className="min-h-screen bg-gray-900 text-white flex flex-col items-center justify-center p-6">
      <h1 className="text-3xl md:text-4xl font-bold mb-6 text-center">
        üéôÔ∏è Text to Podcast Generator
      </h1>

      <input
        type="file"
        accept=".txt"
        onChange={handleFileUpload}
        className="mb-4 text-white"
      />

      <InputSection text={text} setText={setText} onSubmit={handleSubmit} />
      <SpeakerDisplay speaker={speaker} />

      {/* Static audio for now (replace with your file) */}
      <audio
        ref={audioRef}
        controls
        src="/sample.mp3" // Replace with your file path
        className="mt-4"
      />

      <AudioVisualizer audioRef={audioRef} />
    </div>
  );
}

export default App;










import React, { useEffect, useRef } from 'react';

const AudioVisualizer = ({ audioRef }) => {
  const canvasRef = useRef(null);

  useEffect(() => {
    if (!audioRef.current) return;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioCtx.createAnalyser();
    analyser.fftSize = 256;

    const source = audioCtx.createMediaElementSource(audioRef.current);
    source.connect(analyser);
    analyser.connect(audioCtx.destination);

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const canvas = canvasRef.current;
    const canvasCtx = canvas.getContext('2d');

    const draw = () => {
      requestAnimationFrame(draw);

      analyser.getByteFrequencyData(dataArray);

      canvasCtx.fillStyle = '#111827'; // Tailwind slate-900
      canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

      const barWidth = (canvas.width / bufferLength) * 2.5;
      let x = 0;

      for (let i = 0; i < bufferLength; i++) {
        const barHeight = dataArray[i] / 2;

        canvasCtx.fillStyle = '#4ade80'; // Tailwind green-400
        canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);

        x += barWidth + 1;
      }
    };

    draw();

    // Resume context on play (required in some browsers)
    const resume = () => audioCtx.resume();
    audioRef.current.addEventListener('play', resume);

    return () => {
      audioRef.current.removeEventListener('play', resume);
      audioCtx.close();
    };
  }, [audioRef]);

  return (
    <canvas
      ref={canvasRef}
      width={600}
      height={100}
      className="mt-4 border border-green-500 rounded"
    />
  );
};

export default AudioVisualizer;
