<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Input Page</title>
</head>
<body>
  <h2>Enter Query</h2>
  <input type="text" id="queryInput" placeholder="Type something..." />
  <button onclick="submitQuery()">Submit</button>

  <script src="js/index.js"></script>
</body>
</html>




<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Result Page</title>
</head>
<body>
  <h2>Processed Output</h2>
  <p id="displayQuery"></p>

  <script src="js/index.js"></script>
</body>
</html>




// Detect current page
const path = window.location.pathname;

// Page 1: index.html ‚Äì just redirect with raw input
if (path.endsWith("index.html")) {
  window.submitQuery = function () {
    const rawQuery = document.getElementById("queryInput").value.trim();
    if (rawQuery) {
      // Pass raw input to result page (not processed yet)
      const encoded = encodeURIComponent(rawQuery);
      window.location.href = `result.html?q=${encoded}`;
    }
  };
}

// Page 2: result.html ‚Äì receive raw input and process it
if (path.endsWith("result.html")) {
  const params = new URLSearchParams(window.location.search);
  const rawQuery = params.get("q");

  const displayEl = document.getElementById("displayQuery");
  if (rawQuery) {
    const processed = processQuery(rawQuery); // Process here
    displayEl.innerText = `Processed: ${processed}`;
  } else {
    displayEl.innerText = "No query provided.";
  }
}

// Example processing function
function processQuery(input) {
  // Transform raw input (e.g., lowercase ‚Üí uppercase)
  return input.toUpperCase();
}











import React, { useEffect, useRef, useState } from 'react';
import WaveSurfer from 'wavesurfer.js';

const Waveform = ({ audioUrl }) => {
  const waveformRef = useRef(null);
  const wavesurferRef = useRef(null);
  const [isPlaying, setIsPlaying] = useState(false);

  useEffect(() => {
    if (!waveformRef.current) return;

    // Cleanup old instance if re-rendered
    if (wavesurferRef.current) {
      wavesurferRef.current.destroy();
    }

    // Initialize WaveSurfer
    wavesurferRef.current = WaveSurfer.create({
      container: waveformRef.current,
      waveColor: '#94a3b8',
      progressColor: '#4ade80',
      cursorColor: '#22c55e',
      height: 100,
      responsive: true,
      barWidth: 2,
    });

    wavesurferRef.current.load(audioUrl);

    return () => {
      wavesurferRef.current.destroy();
    };
  }, [audioUrl]);

  const togglePlay = () => {
    if (wavesurferRef.current) {
      wavesurferRef.current.playPause();
      setIsPlaying(wavesurferRef.current.isPlaying());
    }
  };

  return (
    <div className="w-full max-w-2xl mx-auto mt-6">
      <div ref={waveformRef} className="rounded border border-green-500" />
      <button
        onClick={togglePlay}
        className="mt-3 px-4 py-2 bg-green-600 hover:bg-green-700 text-white font-semibold rounded"
      >
        {isPlaying ? 'Pause' : 'Play'}
      </button>
    </div>
  );
};

export default Waveform;











import React, { useEffect, useRef } from 'react';

const AudioVisualizer = ({ audioRef }) => {
  const canvasRef = useRef(null);
  const isInitialized = useRef(false); // ‚úÖ Track init

  useEffect(() => {
    if (!audioRef.current || isInitialized.current) return;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioCtx.createAnalyser();
    analyser.fftSize = 256;

    try {
      const source = audioCtx.createMediaElementSource(audioRef.current);
      source.connect(analyser);
      analyser.connect(audioCtx.destination);
    } catch (e) {
      console.error('Audio source already connected:', e);
      return;
    }

    isInitialized.current = true; // ‚úÖ Mark as initialized

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const canvas = canvasRef.current;
    const canvasCtx = canvas.getContext('2d');

    const draw = () => {
      requestAnimationFrame(draw);
      analyser.getByteFrequencyData(dataArray);

      canvasCtx.fillStyle = '#111827';
      canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

      const barWidth = (canvas.width / bufferLength) * 2.5;
      let x = 0;

      for (let i = 0; i < bufferLength; i++) {
        const barHeight = dataArray[i] / 2;
        canvasCtx.fillStyle = '#4ade80';
        canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
        x += barWidth + 1;
      }
    };

    draw();

    const resume = () => audioCtx.resume();
    audioRef.current.addEventListener('play', resume);

    return () => {
      if (audioRef.current) {
        audioRef.current.removeEventListener('play', resume);
      }
      audioCtx.close();
    };
  }, [audioRef]);

  return (
    <canvas
      ref={canvasRef}
      width={600}
      height={100}
      className="mt-4 border border-green-500 rounded"
    />
  );
};

export default AudioVisualizer;








import React, { useState, useEffect, useRef } from 'react';
import './index.css';
import InputSection from './components/InputSection';
import SpeakerDisplay from './components/SpeakerDisplay';
import AudioVisualizer from './components/AudioVisualizer';

function App() {
  const [text, setText] = useState('');
  const [speaker, setSpeaker] = useState(null);
  const [voices, setVoices] = useState([]);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const audioRef = useRef(null); // NEW

  useEffect(() => {
    const handleVoicesChanged = () => {
      const availableVoices = speechSynthesis.getVoices();
      setVoices(availableVoices);
    };

    speechSynthesis.addEventListener('voiceschanged', handleVoicesChanged);
    handleVoicesChanged();

    return () => {
      speechSynthesis.removeEventListener('voiceschanged', handleVoicesChanged);
    };
  }, []);

  const handleSubmit = () => {
    if (!text.trim()) return;

    const sentences = text.split(/(?<=[.!?])\s+/);
    let currentSpeaker = 0;

    const maleVoice = voices.find(v =>
      v.name.toLowerCase().includes('david') || v.name.toLowerCase().includes('male')
    );
    const femaleVoice = voices.find(v =>
      v.name.toLowerCase().includes('zira') || v.name.toLowerCase().includes('female')
    );

    const speak = (index) => {
      if (index >= sentences.length) {
        setSpeaker(null);
        setIsSpeaking(false);
        return;
      }

      const utterance = new SpeechSynthesisUtterance(sentences[index]);

      if (currentSpeaker === 0 && maleVoice) {
        utterance.voice = maleVoice;
      } else if (currentSpeaker === 1 && femaleVoice) {
        utterance.voice = femaleVoice;
      }

      setSpeaker(currentSpeaker);
      currentSpeaker = (currentSpeaker + 1) % 2;

      utterance.onstart = () => setIsSpeaking(true);
      utterance.onend = () => {
        setIsSpeaking(false);
        speak(index + 1);
      };

      speechSynthesis.speak(utterance);
    };

    speak(0);
  };

  const handleFileUpload = (e) => {
    const file = e.target.files[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = (e) => {
      setText(e.target.result);
    };
    reader.readAsText(file);
  };

  return (
    <div className="min-h-screen bg-gray-900 text-white flex flex-col items-center justify-center p-6">
      <h1 className="text-3xl md:text-4xl font-bold mb-6 text-center">
        üéôÔ∏è Text to Podcast Generator
      </h1>

      <input
        type="file"
        accept=".txt"
        onChange={handleFileUpload}
        className="mb-4 text-white"
      />

      <InputSection text={text} setText={setText} onSubmit={handleSubmit} />
      <SpeakerDisplay speaker={speaker} />

      {/* Static audio for now (replace with your file) */}
      <audio
        ref={audioRef}
        controls
        src="/sample.mp3" // Replace with your file path
        className="mt-4"
      />

      <AudioVisualizer audioRef={audioRef} />
    </div>
  );
}

export default App;










import React, { useEffect, useRef } from 'react';

const AudioVisualizer = ({ audioRef }) => {
  const canvasRef = useRef(null);

  useEffect(() => {
    if (!audioRef.current) return;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioCtx.createAnalyser();
    analyser.fftSize = 256;

    const source = audioCtx.createMediaElementSource(audioRef.current);
    source.connect(analyser);
    analyser.connect(audioCtx.destination);

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const canvas = canvasRef.current;
    const canvasCtx = canvas.getContext('2d');

    const draw = () => {
      requestAnimationFrame(draw);

      analyser.getByteFrequencyData(dataArray);

      canvasCtx.fillStyle = '#111827'; // Tailwind slate-900
      canvasCtx.fillRect(0, 0, canvas.width, canvas.height);

      const barWidth = (canvas.width / bufferLength) * 2.5;
      let x = 0;

      for (let i = 0; i < bufferLength; i++) {
        const barHeight = dataArray[i] / 2;

        canvasCtx.fillStyle = '#4ade80'; // Tailwind green-400
        canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);

        x += barWidth + 1;
      }
    };

    draw();

    // Resume context on play (required in some browsers)
    const resume = () => audioCtx.resume();
    audioRef.current.addEventListener('play', resume);

    return () => {
      audioRef.current.removeEventListener('play', resume);
      audioCtx.close();
    };
  }, [audioRef]);

  return (
    <canvas
      ref={canvasRef}
      width={600}
      height={100}
      className="mt-4 border border-green-500 rounded"
    />
  );
};

export default AudioVisualizer;
